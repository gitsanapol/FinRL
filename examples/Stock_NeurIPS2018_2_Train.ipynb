{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [],
   "source": [
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 106        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.439      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -2.05      |\n",
      "|    reward             | 0.14889497 |\n",
      "|    reward_max         | 0.27664906 |\n",
      "|    reward_mean        | 0.0729017  |\n",
      "|    reward_min         | -0.2877896 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.105      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 110        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -32.3      |\n",
      "|    reward             | -1.6407832 |\n",
      "|    reward_max         | 2.5043418  |\n",
      "|    reward_mean        | 0.23038098 |\n",
      "|    reward_min         | -2.0792973 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.01       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -268       |\n",
      "|    reward             | 5.1414695  |\n",
      "|    reward_max         | 5.1414695  |\n",
      "|    reward_mean        | 0.35953778 |\n",
      "|    reward_min         | -3.2069123 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 49.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 106        |\n",
      "|    reward             | 0.49096432 |\n",
      "|    reward_max         | 4.5574656  |\n",
      "|    reward_mean        | 1.6170335  |\n",
      "|    reward_min         | -0.7408161 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.78       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 99         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 542        |\n",
      "|    reward             | -12.682956 |\n",
      "|    reward_max         | 6.8973794  |\n",
      "|    reward_mean        | -3.659322  |\n",
      "|    reward_min         | -15.071192 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 229        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 100         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 92.5        |\n",
      "|    reward             | 0.069733135 |\n",
      "|    reward_max         | 2.8172266   |\n",
      "|    reward_mean        | 0.30458277  |\n",
      "|    reward_min         | -0.5030817  |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.92        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 102         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 34          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -109        |\n",
      "|    reward             | -4.38414    |\n",
      "|    reward_max         | 1.3979179   |\n",
      "|    reward_mean        | -0.23810752 |\n",
      "|    reward_min         | -4.38414    |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 8.51        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 40          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0.0264      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 65.3        |\n",
      "|    reward             | -0.7381241  |\n",
      "|    reward_max         | 2.6907377   |\n",
      "|    reward_mean        | -0.32373965 |\n",
      "|    reward_min         | -1.839416   |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.47        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 184        |\n",
      "|    reward             | -1.2484059 |\n",
      "|    reward_max         | 0.1307447  |\n",
      "|    reward_mean        | -0.5798    |\n",
      "|    reward_min         | -1.2484059 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 23.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 92         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -20        |\n",
      "|    reward             | -6.786732  |\n",
      "|    reward_max         | 0.0467382  |\n",
      "|    reward_mean        | -2.0799184 |\n",
      "|    reward_min         | -6.786732  |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.33       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 90           |\n",
      "|    iterations         | 1100         |\n",
      "|    time_elapsed       | 60           |\n",
      "|    total_timesteps    | 5500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1099         |\n",
      "|    policy_loss        | -543         |\n",
      "|    reward             | 5.848652     |\n",
      "|    reward_max         | 5.848652     |\n",
      "|    reward_mean        | -0.015755545 |\n",
      "|    reward_min         | -11.74265    |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 198          |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 67          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.0211     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -151        |\n",
      "|    reward             | -0.32278278 |\n",
      "|    reward_max         | 3.069909    |\n",
      "|    reward_mean        | 0.71126884  |\n",
      "|    reward_min         | -0.32278278 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 14.4        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 88          |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 73          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -19.7       |\n",
      "|    reward             | -3.5640042  |\n",
      "|    reward_max         | 3.4694097   |\n",
      "|    reward_mean        | -0.18252745 |\n",
      "|    reward_min         | -3.5640042  |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.78        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 87          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 80          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.0097     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -25.8       |\n",
      "|    reward             | 3.244753    |\n",
      "|    reward_max         | 3.244753    |\n",
      "|    reward_mean        | 1.3451746   |\n",
      "|    reward_min         | -0.43539086 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.95        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 86         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 89.8       |\n",
      "|    reward             | 3.813168   |\n",
      "|    reward_max         | 4.1075253  |\n",
      "|    reward_mean        | 0.33219522 |\n",
      "|    reward_min         | -5.602205  |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.24       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -20.2      |\n",
      "|    reward             | 7.8870397  |\n",
      "|    reward_max         | 7.8870397  |\n",
      "|    reward_mean        | 1.2614366  |\n",
      "|    reward_min         | -1.0049084 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.874      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -208       |\n",
      "|    reward             | -1.465231  |\n",
      "|    reward_max         | 3.4815602  |\n",
      "|    reward_mean        | 0.6091739  |\n",
      "|    reward_min         | -1.6459961 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 43.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 84          |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 106         |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | -0.27       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -6.54       |\n",
      "|    reward             | 2.0730867   |\n",
      "|    reward_max         | 2.0730867   |\n",
      "|    reward_mean        | 0.88687223  |\n",
      "|    reward_min         | -0.07887424 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.302       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 113         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -16.9       |\n",
      "|    reward             | 0.02270628  |\n",
      "|    reward_max         | 1.5209552   |\n",
      "|    reward_mean        | -0.19191195 |\n",
      "|    reward_min         | -1.4018036  |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.98        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 120         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -152        |\n",
      "|    reward             | 1.542226    |\n",
      "|    reward_max         | 3.752694    |\n",
      "|    reward_mean        | 1.2646176   |\n",
      "|    reward_min         | -0.99122304 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 17.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -1.2      |\n",
      "|    reward             | 3.607997  |\n",
      "|    reward_max         | 3.607997  |\n",
      "|    reward_mean        | 1.6491215 |\n",
      "|    reward_min         | 0.3520907 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 18.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 82          |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 133         |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | -222        |\n",
      "|    reward             | -4.8877997  |\n",
      "|    reward_max         | -0.36951283 |\n",
      "|    reward_mean        | -3.8231506  |\n",
      "|    reward_min         | -6.045296   |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 46.6        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -2.67e+03 |\n",
      "|    reward             | -16.49976 |\n",
      "|    reward_max         | 47.72924  |\n",
      "|    reward_mean        | 3.7115169 |\n",
      "|    reward_min         | -41.50667 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.29e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 2400        |\n",
      "|    time_elapsed       | 146         |\n",
      "|    total_timesteps    | 12000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | -0.0815     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2399        |\n",
      "|    policy_loss        | 31.5        |\n",
      "|    reward             | 0.7319823   |\n",
      "|    reward_max         | 1.1856265   |\n",
      "|    reward_mean        | 0.3995924   |\n",
      "|    reward_min         | -0.20621884 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.78        |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 81            |\n",
      "|    iterations         | 2500          |\n",
      "|    time_elapsed       | 153           |\n",
      "|    total_timesteps    | 12500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -41.7         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2499          |\n",
      "|    policy_loss        | -55.1         |\n",
      "|    reward             | 0.00013672009 |\n",
      "|    reward_max         | 1.6645408     |\n",
      "|    reward_mean        | 0.6463978     |\n",
      "|    reward_min         | -0.12081669   |\n",
      "|    std                | 1.02          |\n",
      "|    value_loss         | 1.98          |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 159        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | -83        |\n",
      "|    reward             | 2.6250389  |\n",
      "|    reward_max         | 5.0056734  |\n",
      "|    reward_mean        | 1.4103577  |\n",
      "|    reward_min         | -0.6415319 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4          |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 81         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 166        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 2.59       |\n",
      "|    reward             | -0.9085491 |\n",
      "|    reward_max         | 3.1406496  |\n",
      "|    reward_mean        | -0.4643318 |\n",
      "|    reward_min         | -2.0459878 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.223      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 173        |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 327        |\n",
      "|    reward             | 2.3205986  |\n",
      "|    reward_max         | 2.3205986  |\n",
      "|    reward_mean        | 0.23000361 |\n",
      "|    reward_min         | -0.7999886 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 71         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 179         |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | -101        |\n",
      "|    reward             | 2.5966706   |\n",
      "|    reward_max         | 2.5966706   |\n",
      "|    reward_mean        | 0.078267545 |\n",
      "|    reward_min         | -1.5052415  |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 5.53        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 3000        |\n",
      "|    time_elapsed       | 183         |\n",
      "|    total_timesteps    | 15000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2999        |\n",
      "|    policy_loss        | 37.5        |\n",
      "|    reward             | 0.13732196  |\n",
      "|    reward_max         | 0.21145006  |\n",
      "|    reward_mean        | -0.13503735 |\n",
      "|    reward_min         | -0.66219485 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.65        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 82          |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 188         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 102         |\n",
      "|    reward             | -0.16805634 |\n",
      "|    reward_max         | 0.97780424  |\n",
      "|    reward_mean        | -0.15318629 |\n",
      "|    reward_min         | -1.4706929  |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 7.32        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 192        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -1.74      |\n",
      "|    reward             | -0.9085113 |\n",
      "|    reward_max         | 3.0433955  |\n",
      "|    reward_mean        | 1.1187236  |\n",
      "|    reward_min         | -0.9085113 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 18.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 196         |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0.00341     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | -19.8       |\n",
      "|    reward             | -0.52092004 |\n",
      "|    reward_max         | 0.886828    |\n",
      "|    reward_mean        | -0.3763195  |\n",
      "|    reward_min         | -2.4151075  |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.15        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 84          |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 200         |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | 164         |\n",
      "|    reward             | 3.901298    |\n",
      "|    reward_max         | 3.901298    |\n",
      "|    reward_mean        | -0.41497293 |\n",
      "|    reward_min         | -3.8175957  |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 15.7        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 205        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 96.5       |\n",
      "|    reward             | -1.0368418 |\n",
      "|    reward_max         | 2.12608    |\n",
      "|    reward_mean        | 0.27301782 |\n",
      "|    reward_min         | -1.0368418 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 3600        |\n",
      "|    time_elapsed       | 209         |\n",
      "|    total_timesteps    | 18000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3599        |\n",
      "|    policy_loss        | -71.3       |\n",
      "|    reward             | -0.26104552 |\n",
      "|    reward_max         | 2.4117088   |\n",
      "|    reward_mean        | 0.24533504  |\n",
      "|    reward_min         | -0.813737   |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 4.34        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 86          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 213         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 149         |\n",
      "|    reward             | 0.9304586   |\n",
      "|    reward_max         | 0.9304586   |\n",
      "|    reward_mean        | 0.19584951  |\n",
      "|    reward_min         | -0.49330088 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 13.7        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 218        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 166        |\n",
      "|    reward             | 1.0565319  |\n",
      "|    reward_max         | 3.7060466  |\n",
      "|    reward_mean        | 0.97059566 |\n",
      "|    reward_min         | -3.326891  |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 23.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 87          |\n",
      "|    iterations         | 3900        |\n",
      "|    time_elapsed       | 222         |\n",
      "|    total_timesteps    | 19500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3899        |\n",
      "|    policy_loss        | -230        |\n",
      "|    reward             | 1.58274     |\n",
      "|    reward_max         | 1.58274     |\n",
      "|    reward_mean        | 0.039337028 |\n",
      "|    reward_min         | -1.7091787  |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 34.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 227       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -341      |\n",
      "|    reward             | 9.008158  |\n",
      "|    reward_max         | 9.008158  |\n",
      "|    reward_mean        | 3.9458473 |\n",
      "|    reward_min         | 0.5358106 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 71.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 231        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0.143      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -67.4      |\n",
      "|    reward             | 0.43160897 |\n",
      "|    reward_max         | 0.7332951  |\n",
      "|    reward_mean        | 0.37731627 |\n",
      "|    reward_min         | 0.17282015 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 235        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | -39.8      |\n",
      "|    reward             | 1.0645556  |\n",
      "|    reward_max         | 3.532654   |\n",
      "|    reward_mean        | 1.3256818  |\n",
      "|    reward_min         | -0.5502196 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.52       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 240       |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -13.1     |\n",
      "|    reward             | 4.2804213 |\n",
      "|    reward_max         | 4.2804213 |\n",
      "|    reward_mean        | 0.2818005 |\n",
      "|    reward_min         | -3.538382 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.37      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 244        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | 119        |\n",
      "|    reward             | 2.8898268  |\n",
      "|    reward_max         | 2.8898268  |\n",
      "|    reward_mean        | -1.1018937 |\n",
      "|    reward_min         | -3.6466706 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 18.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 248        |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 271        |\n",
      "|    reward             | -1.7544857 |\n",
      "|    reward_max         | 0.79374665 |\n",
      "|    reward_mean        | -0.5682213 |\n",
      "|    reward_min         | -1.7544857 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 51.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 90          |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 253         |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | 224         |\n",
      "|    reward             | 2.7865033   |\n",
      "|    reward_max         | 2.7865033   |\n",
      "|    reward_mean        | 0.040184654 |\n",
      "|    reward_min         | -3.4528394  |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 35.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 257        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -113       |\n",
      "|    reward             | 0.37629944 |\n",
      "|    reward_max         | 3.0168648  |\n",
      "|    reward_mean        | 0.7919917  |\n",
      "|    reward_min         | -1.1104548 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 10         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 91          |\n",
      "|    iterations         | 4800        |\n",
      "|    time_elapsed       | 261         |\n",
      "|    total_timesteps    | 24000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4799        |\n",
      "|    policy_loss        | -205        |\n",
      "|    reward             | -0.94089794 |\n",
      "|    reward_max         | 2.7014225   |\n",
      "|    reward_mean        | 0.6046189   |\n",
      "|    reward_min         | -0.94089794 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 24.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 92         |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 266        |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | -61.3      |\n",
      "|    reward             | 1.2655102  |\n",
      "|    reward_max         | 1.6469687  |\n",
      "|    reward_mean        | 0.40746957 |\n",
      "|    reward_min         | -1.008709  |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 4.7        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 92          |\n",
      "|    iterations         | 5000        |\n",
      "|    time_elapsed       | 270         |\n",
      "|    total_timesteps    | 25000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4999        |\n",
      "|    policy_loss        | -11         |\n",
      "|    reward             | 1.4390299   |\n",
      "|    reward_max         | 2.757684    |\n",
      "|    reward_mean        | -0.57161236 |\n",
      "|    reward_min         | -5.210167   |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 6.61        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 92         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 275        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 212        |\n",
      "|    reward             | -2.6447964 |\n",
      "|    reward_max         | 4.495363   |\n",
      "|    reward_mean        | 1.5498191  |\n",
      "|    reward_min         | -2.6447964 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 35.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 92        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 282       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -173      |\n",
      "|    reward             | 13.391711 |\n",
      "|    reward_max         | 13.391711 |\n",
      "|    reward_mean        | 4.992794  |\n",
      "|    reward_min         | -6.500161 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 187       |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7806805.16\n",
      "total_reward: 6806805.16\n",
      "total_cost: 8856.32\n",
      "total_trades: 51406\n",
      "Sharpe: 1.080\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 91          |\n",
      "|    iterations         | 5300        |\n",
      "|    time_elapsed       | 289         |\n",
      "|    total_timesteps    | 26500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5299        |\n",
      "|    policy_loss        | -90.1       |\n",
      "|    reward             | 0.27623242  |\n",
      "|    reward_max         | 1.5707283   |\n",
      "|    reward_mean        | 0.51428705  |\n",
      "|    reward_min         | -0.42322934 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 4.6         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 91          |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 295         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.2       |\n",
      "|    explained_variance | 0.0219      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | -58.8       |\n",
      "|    reward             | -0.9998574  |\n",
      "|    reward_max         | 1.5970191   |\n",
      "|    reward_mean        | -0.27069566 |\n",
      "|    reward_min         | -3.1108365  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 3.37        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 302        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 209        |\n",
      "|    reward             | 2.402373   |\n",
      "|    reward_max         | 6.374544   |\n",
      "|    reward_mean        | 2.5390527  |\n",
      "|    reward_min         | -1.7153301 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 34         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 309        |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | -145       |\n",
      "|    reward             | 4.102246   |\n",
      "|    reward_max         | 4.8788595  |\n",
      "|    reward_mean        | 1.4412624  |\n",
      "|    reward_min         | -2.2711651 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 17.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 315       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | -993      |\n",
      "|    reward             | -9.255028 |\n",
      "|    reward_max         | 23.95434  |\n",
      "|    reward_mean        | -5.738885 |\n",
      "|    reward_min         | -31.53399 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 570       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 322        |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | -0.929     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | 30.1       |\n",
      "|    reward             | 3.7457094  |\n",
      "|    reward_max         | 5.341521   |\n",
      "|    reward_mean        | 0.89907604 |\n",
      "|    reward_min         | -7.0162215 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 328        |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 55.2       |\n",
      "|    reward             | -0.5115396 |\n",
      "|    reward_max         | 0.9692764  |\n",
      "|    reward_mean        | 0.25550908 |\n",
      "|    reward_min         | -0.7830049 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.36       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 335        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 277        |\n",
      "|    reward             | -0.9022603 |\n",
      "|    reward_max         | 4.2005234  |\n",
      "|    reward_mean        | 0.4487342  |\n",
      "|    reward_min         | -3.1211164 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 53.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 341        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | -51.4      |\n",
      "|    reward             | -5.6876345 |\n",
      "|    reward_max         | 6.595557   |\n",
      "|    reward_mean        | -1.469457  |\n",
      "|    reward_min         | -8.660135  |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.51       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 348        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | 10.2       |\n",
      "|    reward             | 0.32049274 |\n",
      "|    reward_max         | 1.695732   |\n",
      "|    reward_mean        | -2.178283  |\n",
      "|    reward_min         | -6.0575504 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.66       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 354        |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | 1.18e+03   |\n",
      "|    reward             | 14.230099  |\n",
      "|    reward_max         | 14.230099  |\n",
      "|    reward_mean        | 1.9508442  |\n",
      "|    reward_min         | -10.872334 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.03e+03   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 88          |\n",
      "|    iterations         | 6400        |\n",
      "|    time_elapsed       | 361         |\n",
      "|    total_timesteps    | 32000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.2       |\n",
      "|    explained_variance | -0.253      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6399        |\n",
      "|    policy_loss        | 39.1        |\n",
      "|    reward             | 1.5337281   |\n",
      "|    reward_max         | 1.5337281   |\n",
      "|    reward_mean        | 0.60667884  |\n",
      "|    reward_min         | -0.60451585 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 1.32        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 367        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 68         |\n",
      "|    reward             | -4.3627152 |\n",
      "|    reward_max         | 5.089546   |\n",
      "|    reward_mean        | -0.9058697 |\n",
      "|    reward_min         | -4.3627152 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 374        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -48.2      |\n",
      "|    reward             | 0.48769075 |\n",
      "|    reward_max         | 1.7491184  |\n",
      "|    reward_mean        | 0.39129263 |\n",
      "|    reward_min         | -1.6699541 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 6.68       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 381        |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | -0.000291  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | -1.45e+03  |\n",
      "|    reward             | -12.865579 |\n",
      "|    reward_max         | 17.794037  |\n",
      "|    reward_mean        | 1.9081234  |\n",
      "|    reward_min         | -12.865579 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.52e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 387        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | -166       |\n",
      "|    reward             | 0.71224326 |\n",
      "|    reward_max         | 6.9638486  |\n",
      "|    reward_mean        | 1.268115   |\n",
      "|    reward_min         | -3.9043398 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 31.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 87        |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 394       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -457      |\n",
      "|    reward             | -8.882997 |\n",
      "|    reward_max         | 11.862731 |\n",
      "|    reward_mean        | 2.3381548 |\n",
      "|    reward_min         | -8.882997 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 267       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 87          |\n",
      "|    iterations         | 7000        |\n",
      "|    time_elapsed       | 400         |\n",
      "|    total_timesteps    | 35000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.2       |\n",
      "|    explained_variance | -0.0874     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6999        |\n",
      "|    policy_loss        | 55.4        |\n",
      "|    reward             | -0.13382462 |\n",
      "|    reward_max         | 2.4703739   |\n",
      "|    reward_mean        | 0.71661735  |\n",
      "|    reward_min         | -0.7490699  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 2.03        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 7100       |\n",
      "|    time_elapsed       | 407        |\n",
      "|    total_timesteps    | 35500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7099       |\n",
      "|    policy_loss        | 58.8       |\n",
      "|    reward             | -0.1590894 |\n",
      "|    reward_max         | 2.1653194  |\n",
      "|    reward_mean        | 0.64857864 |\n",
      "|    reward_min         | -0.1590894 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 4.66       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 87        |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 413       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -274      |\n",
      "|    reward             | 0.7829578 |\n",
      "|    reward_max         | 3.1199281 |\n",
      "|    reward_mean        | 1.8561118 |\n",
      "|    reward_min         | 0.7131336 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 48.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 86         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 420        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -192       |\n",
      "|    reward             | -2.2955062 |\n",
      "|    reward_max         | 18.427654  |\n",
      "|    reward_mean        | 2.5456052  |\n",
      "|    reward_min         | -5.2222557 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 25         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 86          |\n",
      "|    iterations         | 7400        |\n",
      "|    time_elapsed       | 426         |\n",
      "|    total_timesteps    | 37000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.2       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7399        |\n",
      "|    policy_loss        | 380         |\n",
      "|    reward             | -5.523793   |\n",
      "|    reward_max         | 5.1887665   |\n",
      "|    reward_mean        | -0.44591054 |\n",
      "|    reward_min         | -5.523793   |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 102         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 86         |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 433        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 1.07e+03   |\n",
      "|    reward             | -10.710269 |\n",
      "|    reward_max         | 5.5329223  |\n",
      "|    reward_mean        | -3.7745674 |\n",
      "|    reward_min         | -10.710269 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 670        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 86         |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 439        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | -119       |\n",
      "|    reward             | 0.9044327  |\n",
      "|    reward_max         | 2.0217786  |\n",
      "|    reward_mean        | 0.61338085 |\n",
      "|    reward_min         | -1.5883253 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 86         |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 446        |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | -104       |\n",
      "|    reward             | 1.530832   |\n",
      "|    reward_max         | 2.8562722  |\n",
      "|    reward_mean        | 1.1602662  |\n",
      "|    reward_min         | -1.3449589 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.19       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 86          |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 452         |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | -19.6       |\n",
      "|    reward             | 0.9663002   |\n",
      "|    reward_max         | 2.4717891   |\n",
      "|    reward_mean        | 1.0135063   |\n",
      "|    reward_min         | -0.73231685 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 2.39        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 86         |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 458        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | 38.6       |\n",
      "|    reward             | 6.258552   |\n",
      "|    reward_max         | 6.258552   |\n",
      "|    reward_mean        | 1.4441563  |\n",
      "|    reward_min         | -0.6200948 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 47.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 86          |\n",
      "|    iterations         | 8000        |\n",
      "|    time_elapsed       | 465         |\n",
      "|    total_timesteps    | 40000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7999        |\n",
      "|    policy_loss        | -252        |\n",
      "|    reward             | -3.113796   |\n",
      "|    reward_max         | 5.476019    |\n",
      "|    reward_mean        | -0.75957096 |\n",
      "|    reward_min         | -6.3715053  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 71.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 8100       |\n",
      "|    time_elapsed       | 471        |\n",
      "|    total_timesteps    | 40500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8099       |\n",
      "|    policy_loss        | 348        |\n",
      "|    reward             | 9.435927   |\n",
      "|    reward_max         | 9.905439   |\n",
      "|    reward_mean        | -2.6988654 |\n",
      "|    reward_min         | -22.16255  |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 79.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 477         |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | -146        |\n",
      "|    reward             | 0.1547046   |\n",
      "|    reward_max         | 0.38819835  |\n",
      "|    reward_mean        | 0.178523    |\n",
      "|    reward_min         | -0.24595594 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 11.5        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 8300        |\n",
      "|    time_elapsed       | 483         |\n",
      "|    total_timesteps    | 41500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8299        |\n",
      "|    policy_loss        | -62.1       |\n",
      "|    reward             | -2.2348404  |\n",
      "|    reward_max         | 1.9132785   |\n",
      "|    reward_mean        | -0.08967479 |\n",
      "|    reward_min         | -2.2348404  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 2.11        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 8400        |\n",
      "|    time_elapsed       | 490         |\n",
      "|    total_timesteps    | 42000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8399        |\n",
      "|    policy_loss        | 14.7        |\n",
      "|    reward             | -0.96506363 |\n",
      "|    reward_max         | 1.6384324   |\n",
      "|    reward_mean        | -1.6090245  |\n",
      "|    reward_min         | -4.7289877  |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.459       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 496         |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -346        |\n",
      "|    reward             | -0.29401645 |\n",
      "|    reward_max         | 6.8139744   |\n",
      "|    reward_mean        | 2.2399282   |\n",
      "|    reward_min         | -0.29401645 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 76.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 502        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 269        |\n",
      "|    reward             | -20.230286 |\n",
      "|    reward_max         | 16.804518  |\n",
      "|    reward_mean        | 1.3256384  |\n",
      "|    reward_min         | -20.230286 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 57.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 509        |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | -3.47      |\n",
      "|    reward             | 0.4320347  |\n",
      "|    reward_max         | 1.6396383  |\n",
      "|    reward_mean        | 0.75311637 |\n",
      "|    reward_min         | 0.21346927 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.02       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 8800        |\n",
      "|    time_elapsed       | 515         |\n",
      "|    total_timesteps    | 44000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8799        |\n",
      "|    policy_loss        | -62.7       |\n",
      "|    reward             | 0.4940848   |\n",
      "|    reward_max         | 0.4940848   |\n",
      "|    reward_mean        | -0.07884119 |\n",
      "|    reward_min         | -0.9822231  |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.27        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 521        |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | 70.2       |\n",
      "|    reward             | 2.3595326  |\n",
      "|    reward_max         | 2.3595326  |\n",
      "|    reward_mean        | -0.372179  |\n",
      "|    reward_min         | -1.9893069 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.69       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 527        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 27.7       |\n",
      "|    reward             | -1.3062522 |\n",
      "|    reward_max         | 3.8228323  |\n",
      "|    reward_mean        | 0.40261737 |\n",
      "|    reward_min         | -1.42607   |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 8.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 534        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | -20.4      |\n",
      "|    reward             | 1.4075464  |\n",
      "|    reward_max         | 1.459045   |\n",
      "|    reward_mean        | 0.46022928 |\n",
      "|    reward_min         | -1.0814227 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.914      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 540        |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | -44.9      |\n",
      "|    reward             | 5.412038   |\n",
      "|    reward_max         | 5.412038   |\n",
      "|    reward_mean        | -0.7837366 |\n",
      "|    reward_min         | -7.375119  |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 11.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 546        |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | -82.8      |\n",
      "|    reward             | 1.449943   |\n",
      "|    reward_max         | 2.1642947  |\n",
      "|    reward_mean        | 0.33451194 |\n",
      "|    reward_min         | -2.8501039 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.75       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 553        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 136        |\n",
      "|    reward             | -0.3648397 |\n",
      "|    reward_max         | 3.0740983  |\n",
      "|    reward_mean        | 0.39587316 |\n",
      "|    reward_min         | -2.3675141 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 11.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 559        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 137        |\n",
      "|    reward             | 0.5253892  |\n",
      "|    reward_max         | 1.4352291  |\n",
      "|    reward_mean        | 0.22423613 |\n",
      "|    reward_min         | -1.1692696 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 11.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 565        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -286       |\n",
      "|    reward             | -1.0472598 |\n",
      "|    reward_max         | 2.6436954  |\n",
      "|    reward_mean        | 0.8025634  |\n",
      "|    reward_min         | -1.0472598 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 43.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 571        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 113        |\n",
      "|    reward             | 0.28053048 |\n",
      "|    reward_max         | 3.6964936  |\n",
      "|    reward_mean        | 1.7354187  |\n",
      "|    reward_min         | 0.28053048 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 9.58       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 9800       |\n",
      "|    time_elapsed       | 578        |\n",
      "|    total_timesteps    | 49000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9799       |\n",
      "|    policy_loss        | 251        |\n",
      "|    reward             | 3.8625221  |\n",
      "|    reward_max         | 6.6981707  |\n",
      "|    reward_mean        | 0.29346302 |\n",
      "|    reward_min         | -6.307829  |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 64.4       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 84           |\n",
      "|    iterations         | 9900         |\n",
      "|    time_elapsed       | 584          |\n",
      "|    total_timesteps    | 49500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.7        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9899         |\n",
      "|    policy_loss        | -16.4        |\n",
      "|    reward             | -0.06675278  |\n",
      "|    reward_max         | 0.77229416   |\n",
      "|    reward_mean        | -0.073969506 |\n",
      "|    reward_min         | -1.2027911   |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.356        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 84          |\n",
      "|    iterations         | 10000       |\n",
      "|    time_elapsed       | 591         |\n",
      "|    total_timesteps    | 50000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9999        |\n",
      "|    policy_loss        | 38.1        |\n",
      "|    reward             | -0.18344378 |\n",
      "|    reward_max         | 0.7269467   |\n",
      "|    reward_mean        | 0.09327335  |\n",
      "|    reward_min         | -0.43824956 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 1.3         |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cuda device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n",
      "Logging Error: 'rollout_buffer'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_ddpg \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_ddpg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mddpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m if_using_ddpg \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LOL\\anaconda3\\envs\\FinRL\\lib\\site-packages\\finrl\\agents\\stablebaselines3\\models.py:143\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[1;34m(model, tb_log_name, total_timesteps, callbacks)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[0;32m    138\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     callbacks: Type[BaseCallback] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    142\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCallbackList\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    151\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\LOL\\anaconda3\\envs\\FinRL\\lib\\site-packages\\stable_baselines3\\ddpg\\ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[0;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LOL\\anaconda3\\envs\\FinRL\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LOL\\anaconda3\\envs\\FinRL\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LOL\\anaconda3\\envs\\FinRL\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:203\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    202\u001b[0m polyak_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n\u001b[1;32m--> 203\u001b[0m \u001b[43mpolyak_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Copy running stats, see GH issue #996\u001b[39;00m\n\u001b[0;32m    205\u001b[0m polyak_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_batch_norm_stats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_batch_norm_stats_target, \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LOL\\anaconda3\\envs\\FinRL\\lib\\site-packages\\stable_baselines3\\common\\utils.py:475\u001b[0m, in \u001b[0;36mpolyak_update\u001b[1;34m(params, target_params, tau)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# zip does not raise an exception if length of parameters does not match.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param, target_param \u001b[38;5;129;01min\u001b[39;00m zip_strict(params, target_params):\n\u001b[1;32m--> 475\u001b[0m         \u001b[43mtarget_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m         th\u001b[38;5;241m.\u001b[39madd(target_param\u001b[38;5;241m.\u001b[39mdata, param\u001b[38;5;241m.\u001b[39mdata, alpha\u001b[38;5;241m=\u001b[39mtau, out\u001b[38;5;241m=\u001b[39mtarget_param\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FinRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
